---
trigger: always_on
---

Purpose:
This project, Playground, is a modular and experimental environment designed to explore, compare, and explain different implementation strategies‚Äîparticularly in conversational or interactive applications such as chatbots, workflows, or AI-powered tools.

Audience:

    Primarily non-technical or semi-technical stakeholders who need to understand how the system works, how users interact with it, and how each feature affects cost and performance.

    Secondary audience includes technical users looking to test and iterate quickly.

‚úÖ Core Principles:

    Experimental Flexibility
    Each module or feature in Playground may follow a different implementation strategy (e.g., API-first, edge-compute, serverless, LLM integration). All strategies are treated equally for comparison and analysis.

    Clarity Over Complexity
    Interfaces and documentation must prioritize clarity, offering explainers, diagrams, and real-time feedback where possible to make complexity more accessible.

    UX-Driven Evaluation
    The experience of the end user is a first-class metric. All experiments should offer a clear user flow, a way to measure friction, and space for users to give feedback on ease-of-use.

    Cost Awareness
    Each operation or interaction must include transparent cost metrics, either real (if production-ready) or estimated (if in simulation). Playground users should be able to:

        View cost per request or cost per strategy.

        Understand trade-offs between latency, accuracy, resource use, and financial cost.

    Interoperability & Extendability
    Playground is modular. Any component (chat model, UI widget, backend logic) can be replaced, mocked, or extended without breaking the rest of the system.

üì¶ Instruction for Implementers:

    Always label and explain what kind of strategy is being tested (e.g., RAG vs. fine-tuned model, SSR vs. SPA).

    Provide step-by-step UX walkthroughs alongside each experiment.

    Log each user interaction in a way that allows tracing of both UX satisfaction and resource/cost usage.

    Use feature flags to activate/deactivate specific implementations easily.

    Ensure all visual elements are accessible and all interactions are traceable for analysis and refinement.

üåê Outputs Playground Should Support:

    Comparative dashboards between strategies.

    Exportable UX feedback reports.

    Cost analysis per feature or per user flow.

    Optional telemetry and heatmaps (toggleable for privacy).